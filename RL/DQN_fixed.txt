{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vbrBxWc8O2"
      },
      "source": [
        "# Licence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTl-ivwIcS38"
      },
      "outputs": [],
      "source": [
        "#Licence\n",
        "#Copyright 2021 Google LLC.\n",
        "#SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErkTGyQ0Qase"
      },
      "source": [
        "#Deep Q-Network\n",
        "\n",
        "This practical aims at implementing, from scratch, a working <a href=\"https://arxiv.org/abs/1312.5602\">Deep Q-learning</a> agent on a simple Catch game."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVaNs4WEQekY"
      },
      "source": [
        "##Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKP2o0QuQhpd"
      },
      "source": [
        "1. [Acme](https://github.com/deepmind/acme) is a library of reinforcement learning (RL) agents and agent building blocks.\n",
        "\n",
        "2. [Haiku](https://github.com/deepmind/dm-haiku) is a neural network library for JAX that we saw in the previous practical.\n",
        "\n",
        "3. [dm_env](https://github.com/deepmind/dm_env): DeepMind Environment API, which will be covered in more details in the [Environment subsection](https://colab.research.google.com/drive/1oKyyhOFAFSBTpVnmuOm9HXh5D5ekqhh5#scrollTo=I6KuVGSk4uc9) below.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gDZSbrcjQkJ5"
      },
      "outputs": [],
      "source": [
        "#@title Installations  { form-width: \"30%\" }\n",
        "\n",
        "%pip install git+https://github.com/deepmind/acme.git#egg=dm-acme[jax,tf,envs]\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sBqd3jWPQ6YJ"
      },
      "outputs": [],
      "source": [
        "#@title Imports  { form-width: \"30%\" }\n",
        "\n",
        "import bsuite.environments.catch as dm_catch\n",
        "import chex\n",
        "import dm_env\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import tree\n",
        "from typing import Callable\n",
        "\n",
        "# Filter out warnings as they are distracting.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6WHNe9WQ8-g"
      },
      "source": [
        "# Catch environment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmhfHAMRRBnJ"
      },
      "source": [
        "We here focus on a very simple environment: Catch!\n",
        "\n",
        "The agent must move a paddle to intercept falling balls. Falling balls only\n",
        "move downwards on the column they are in.\n",
        "The observation is an array shape (rows, columns), with binary values:\n",
        "zero if a space is empty; 1 if it contains the paddle or a ball.\n",
        "The actions are discrete, and by default there are three available:\n",
        "stay, move left, and move right.\n",
        "The episode terminates when the ball reaches the bottom of the screen.\n",
        "\n",
        "The first thing you are going to do is implement a random policy on this environment, i.e. a policy that returns a uniformly random action, whatever the observation received.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9cjyGYJZo0iC"
      },
      "outputs": [],
      "source": [
        "# @title Interacting with the environment.\n",
        "\n",
        "environment = dm_catch.Catch(rows=5)\n",
        "\n",
        "# Create random policy.\n",
        "def random_policy(obs: chex.Array) -> chex.Array:\n",
        "  return np.random.randint(environment.action_spec().num_values)\n",
        "\n",
        "# Simple map from action index to a descriptive string.\n",
        "int2action = {0: \"left\", 1: \"no-op\", 2: \"right\"}\n",
        "\n",
        "# Reset any counts and start the environment.\n",
        "timestep = environment.reset()\n",
        "\n",
        "# Run an episode.\n",
        "while not timestep.last():\n",
        "\n",
        "  print(timestep.observation)\n",
        "\n",
        "  # Generate an action from the agent's policy and step the environment.\n",
        "  action = random_policy(timestep.observation)\n",
        "  timestep = environment.step(action)\n",
        "  print(f\"\\nAction: {int2action[action]}\\n\")\n",
        "\n",
        "\n",
        "print(timestep.observation)\n",
        "print(f\"Episode ended with final reward: {timestep.reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPmj7cfZ5Ych"
      },
      "source": [
        "# Deep Q-learning on Catch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESaSQvhl5fBV"
      },
      "source": [
        "We are going to implement Deep Q-learning on Catch. For this, we need three core components:\n",
        "  - a replay buffer,\n",
        "  - a function approximator (here, a neural network),\n",
        "  - a target network.\n",
        "\n",
        "We will then put these components together into a training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtlEEWqSlq5"
      },
      "source": [
        "## Uniform replay buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKlKSbmBInD"
      },
      "source": [
        "As a first step, let's implement a _uniform replay buffer_.\n",
        "A replay buffer stores transitions from the environment, i.e. when the agent interacts with the environment and produces trajectories\n",
        "\n",
        "$$(s_0, a_0, r_0, \\mathrm{done}_0, s_1, a_1, r_1, \\mathrm{done}_1, \\ldots),$$\n",
        "\n",
        "the replay buffer will be used to store a maximum of $N$ _transitions_ from the trajectories, i.e. t-tuples of the form\n",
        "$$(s_t, a_t, r_t, \\mathrm{done}_t, s_{t+1})\\cdot$$\n",
        "From those transitions, the replay buffer will allow us to randomly sample individual or batches of transitions. Without a replay buffer, when training our agents, we would always use consecutive transitions coming from the trajectory. Those transitions are extremely correlated, and neural networks trained with gradient descent tend to not play well with very correlated samples. The replay buffer is one of the core component that allow DQN to scale up to complex environment.\n",
        "\n",
        "Your goal is thus to implement a class ReplayBuffer that store transitions from trajectories and resample them a posteriori. Your replay buffer must have the following properties:\n",
        " - A limited capacity to store $\\kappa$ tuples.\n",
        " - when a tuple is added, if there is no space left in memory, it replaces the oldest tuple in the replay buffer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FoYvBLMaMF7s"
      },
      "outputs": [],
      "source": [
        "#@title **[Implement]** Uniform Replay Buffer { form-width: \"30%\" }\n",
        "\n",
        "@chex.dataclass\n",
        "class Transition:\n",
        "  observation: chex.Array\n",
        "  action: chex.Array\n",
        "  reward: chex.Array\n",
        "  done: chex.Array\n",
        "  next_observation: chex.Array\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "  \"\"\"Fixed-size buffer to store transition tuples.\"\"\"\n",
        "\n",
        "  def __init__(self, buffer_capacity: int):\n",
        "      \"\"\"Initialize a ReplayBuffer object.\n",
        "      Args:\n",
        "          buffer_capacity (int): maximum allowed size of the replay buffer.\n",
        "      \"\"\"\n",
        "      self._memory = list()\n",
        "      self._maxlen = buffer_capacity\n",
        "\n",
        "  @property\n",
        "  def size(self) -> int:\n",
        "    # Return the current number of elements in the buffer.\n",
        "    return len(self._memory)\n",
        "\n",
        "  def add(self, transition: Transition) -> None:\n",
        "      \"\"\"Add a new transition to memory.\"\"\"\n",
        "      # Your code here !\n",
        "      if self.size >= self._maxlen:\n",
        "        self._memory.pop(0)\n",
        "      self._memory.append(transition)\n",
        "\n",
        "  def sample_batch(self, batch_size: int) -> Transition:\n",
        "    \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "    assert len(self._memory) >= batch_size, 'Insuficient number of transitions in replay buffer'\n",
        "    # Your code here !\n",
        "    transitions: list[Transition] = random.choices(self._memory, k=batch_size)\n",
        "    return tree.map_structure(lambda *x: np.stack(x), *transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vwLYRgo8oNJc"
      },
      "outputs": [],
      "source": [
        "# @title Test the replay buffer\n",
        "\n",
        "batch_size = 32\n",
        "buffer = ReplayBuffer(100)\n",
        "\n",
        "for _ in range(batch_size):\n",
        "  ts = environment.reset()\n",
        "\n",
        "  buffer.add(\n",
        "      Transition(\n",
        "          observation=ts.observation,\n",
        "          action=1,\n",
        "          reward=ts.reward or 0.,\n",
        "          done=ts.last(),\n",
        "          next_observation=ts.observation,\n",
        "      )\n",
        "  )\n",
        "\n",
        "assert buffer.sample_batch(batch_size).observation.shape[0] == batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHPOBsmvBTKz"
      },
      "source": [
        "## Implementing DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO5NSfxvBW2N"
      },
      "source": [
        "We are now ready to get to the DQN implementation. First, let's define the network. Catch is a small environment, whose inputs are spatial dimension Row * Column * 1, with 0/1 inputs. What could be a good network to handle this environment? Remember that when you approximate a Q-function, the Q network should take in an action, and output as many values as there are available actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxfddPfnQBSU"
      },
      "outputs": [],
      "source": [
        "#@title **[Implement]** Catch Network { form-width: \"30%\" }\n",
        "num_rows = 5\n",
        "num_columns = 5\n",
        "\n",
        "environment = dm_catch.Catch(rows=num_rows, columns=num_columns)\n",
        "num_actions = environment.action_spec().num_values\n",
        "\n",
        "def catch_network(x: chex.Array):\n",
        "  # Your code here !\n",
        "  out = hk.Flatten()(x)  # For example: [B, 5, 5] -> [B, 25].\n",
        "  out = hk.Linear(64)(out)\n",
        "  out = jax.nn.relu(out)\n",
        "  out = hk.Linear(num_actions)(out)\n",
        "  return out\n",
        "\n",
        "# Create the neural network pure functions.\n",
        "dqn_network = hk.without_apply_rng(hk.transform(catch_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7oStESGQP4E"
      },
      "source": [
        "Next, we will define all the parts of our agent in the `DQNAgent` class below.\n",
        "\n",
        "1. First initialize the state of learning `self._learner_state` in `__init__`. This state includes:\n",
        "    - the parameters of our online network,\n",
        "    - the parameters of our target network, and\n",
        "    - the state of the optimizer (e.g. Adam first and second moments).\n",
        "\n",
        "\n",
        "2. Second, fill in the `select_action` method, that outputs an action selected using an $\\varepsilon$ policy using the current estimate of the Q-function.\n",
        "\n",
        "3. Third, implement the `_loss_fn` method. To that end, remember that the DQN loss function can be written as\n",
        "$$L_i (\\theta_i) = \\mathbb{E}_{\\color{red}{s},\\color{blue}{a} \\sim \\rho(\\cdot)} \\left[ \\left( y_i - Q(\\color{red}{s},\\color{blue}{a} ;\\theta_i) \\right)^2\\right]$$\n",
        "where the target $y_i$ is computed using a bootstrap value computed from Q-value network with target parameters:\n",
        "$$ y_i = \\mathbb{E}_{\\color{red}{s'} \\sim \\mathcal{E}} \\left[ \\color{green}{r} + \\gamma \\max_{\\color{blue}{a'} \\in \\color{blue}{\\mathcal{A}}} Q(\\color{red}{s'}, \\color{blue}{a'} ; \\theta^{\\text{target}}_i) \\; | \\; \\color{red}{s}, \\color{blue}{a} \\right] $$\n",
        "\n",
        "4. Finally implement the `_update_fn`. This function first computes the gradient of your loss function at the current batch, then passes this gradient to an optimizer (here Adam) to get an update. Finally you need to apply this update to the current parameters. This is done using the `optax` package, and you can find an example on how to use it on this [github page](https://github.com/deepmind/optax/blob/master/README.md) and in this [colab](https://github.com/deepmind/optax/blob/master/examples/quick_start.ipynb).\n",
        "\n",
        "Some hints for Step 3 (implementing the loss function):\n",
        " - Do not forget to *not* bootstrap the final Q-value!\n",
        " - The inputs are batched, so you will need batched indexing, i.e. accessing the Q-function of a certain action for each element in your batch of actions.\n",
        "\n",
        "Once all these steps are completed, you can test your agent on the catch environment below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqMM8K5qTAEj"
      },
      "outputs": [],
      "source": [
        "#@title **[Implement]** DQN agent\n",
        "\n",
        "@chex.dataclass\n",
        "class LearnerState:\n",
        "  \"\"\"\"Container for all variables needed for training.\"\"\"\n",
        "  online_params: hk.Params\n",
        "  target_params: hk.Params\n",
        "  opt_state: optax.OptState\n",
        "\n",
        "class DQNAgent:\n",
        "  \"\"\"Implementation of the DQN agent.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      init_params_fn: Callable[[jax.random.KeyArray], hk.Params],\n",
        "      network_apply_fn: Callable[[hk.Params, chex.Array], chex.Array],\n",
        "      optimizer: optax.GradientTransformation,\n",
        "      gamma: float,\n",
        "      epsilon: float,\n",
        "      num_actions: int,\n",
        "      buffer_capacity: int,\n",
        "      batch_size: int,\n",
        "      target_ema: float,\n",
        "      seed: int = 0,\n",
        "  ) -> None:\n",
        "    \"\"\"Initializes the DQN agent.\n",
        "\n",
        "    Args:\n",
        "      init_params_fn: the pure function which initializes the network parameters.\n",
        "      network_apply_fn: the pure function corresponding to the desired DQN network.\n",
        "      optimizer: the optimizer used to minimize the DQN loss.\n",
        "      gamma: the agent's discount factor.\n",
        "      epsilon: probability to perform a random exploration when picking a new action.\n",
        "      num_actions: number of actions in the environment's action space.\n",
        "      buffer_capacity: capacity of the replay buffer.\n",
        "      batch_size: batch size when updating the online network.\n",
        "      target_ema: coefficient for the exponential moving average computation of\n",
        "        the target network parameters.\n",
        "      seed: seed of the random generator.\n",
        "    \"\"\"\n",
        "    self._gamma = gamma\n",
        "    self._epsilon = epsilon\n",
        "    self._num_actions = num_actions\n",
        "    self._batch_size = batch_size\n",
        "    self._target_ema = target_ema\n",
        "\n",
        "    # Set the neural network and optimizer.\n",
        "    self._network_apply_fn = network_apply_fn\n",
        "    self._optimizer = optimizer\n",
        "\n",
        "    # Initialize the replay buffer.\n",
        "    self._buffer = ReplayBuffer(buffer_capacity)\n",
        "\n",
        "    # Always store the current observation so we can create transitions.\n",
        "    self._observation = None\n",
        "\n",
        "    # Initialize the network's parameters.\n",
        "    params = init_params_fn(jax.random.PRNGKey(seed))\n",
        "\n",
        "    # Initialize the learner state.\n",
        "    # Your code here !\n",
        "    self._learner_state = LearnerState(\n",
        "        online_params=params,\n",
        "        target_params=jax.tree_map(jnp.copy, params),\n",
        "        opt_state=self._optimizer.init(params),\n",
        "    )\n",
        "\n",
        "    # JIT the update step.\n",
        "    self._update = jax.jit(self._update_fn)\n",
        "\n",
        "  def observe_first(self, observation: chex.Array) -> None:\n",
        "    self._observation = observation\n",
        "\n",
        "  def select_action(\n",
        "      self,\n",
        "      observation: chex.Array,\n",
        "      eval: bool,\n",
        "  ) -> chex.Array:\n",
        "    \"\"\"Picks the next action using an epsilon greedy policy.\n",
        "\n",
        "    Args:\n",
        "      obersation: observed state of the environment.\n",
        "      eval: if True the agent is acting in evaluation mode (which means it only\n",
        "        acts according to the best policy it knows.)\n",
        "    \"\"\"\n",
        "    # Fill in this function to act using an epsilon-greedy policy.\n",
        "    # Your code here !\n",
        "    if eval or np.random.uniform() > self._epsilon:\n",
        "      # The network expects a batch dimension so we add one here.\n",
        "      observation = jnp.expand_dims(observation, axis=0)\n",
        "      # Greedy action selection.\n",
        "      q_values = self._network_apply_fn(\n",
        "          self._learner_state.target_params,\n",
        "          observation\n",
        "      )\n",
        "      # Remove the batch dimension that was added above.\n",
        "      q_values = jnp.squeeze(q_values, axis=0)\n",
        "      action = jnp.argmax(q_values, axis=-1)\n",
        "    else:\n",
        "      # Random action selection.\n",
        "      action = np.random.randint(self._num_actions)\n",
        "    \n",
        "    return action\n",
        "\n",
        "  def _loss_fn(\n",
        "      self,\n",
        "      online_params: hk.Params,\n",
        "      target_params: hk.Params,\n",
        "      transition: Transition,\n",
        "  ) -> chex.Array:\n",
        "      \"\"\"Computes the Q-learning loss\n",
        "\n",
        "      Args:\n",
        "        online_params: parameters of the online network.\n",
        "        target_params: parameters of the target network.\n",
        "        transition: container of transition quantities (s, a, r, done, s')\n",
        "      Returns:\n",
        "        The Q-learning loss.\n",
        "      \"\"\"\n",
        "      # Your code here !\n",
        "      target_q_values = self._network_apply_fn(\n",
        "          target_params,\n",
        "          transition.next_observation,\n",
        "      )\n",
        "      y = jnp.where(\n",
        "          transition.done,\n",
        "          transition.reward,\n",
        "          transition.reward + self._gamma * jnp.max(target_q_values, axis=-1),\n",
        "      )\n",
        "      online_q_values = self._network_apply_fn(\n",
        "          online_params,\n",
        "          transition.observation,\n",
        "      )\n",
        "      online_q_value_at_action = jnp.take_along_axis(\n",
        "          online_q_values,\n",
        "          transition.action[:, None],\n",
        "          axis=-1,\n",
        "      )\n",
        "      online_q_value_at_action = jnp.squeeze(\n",
        "          online_q_value_at_action,\n",
        "          axis=-1,\n",
        "      )\n",
        "      loss = 0.5 * jnp.mean(jnp.square(y - online_q_value_at_action))\n",
        "\n",
        "      return loss\n",
        "\n",
        "  def _update_fn(\n",
        "      self,\n",
        "      state: LearnerState,\n",
        "      batch: Transition,    \n",
        "  ) -> tuple[LearnerState, chex.Array]:\n",
        "    \"\"\"Get the next learner state given the current batch of transitions.\n",
        "\n",
        "    Args:\n",
        "      state: the current learner state.\n",
        "      batch: batch of transitions (st, at, rt, done_t, stp1)\n",
        "    Returns:\n",
        "      A tuple of:\n",
        "        - the updated learner state, and\n",
        "        - the loss incurred by the previous learner state given the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute gradients\n",
        "    # Your code here !\n",
        "    loss, grad = jax.value_and_grad(self._loss_fn)(\n",
        "        state.online_params,\n",
        "        target_params=state.target_params,\n",
        "        transition=batch\n",
        "    )\n",
        "\n",
        "    # Apply gradients\n",
        "    # Your code here !\n",
        "    updates, opt_state = self._optimizer.update(grad, state.opt_state)\n",
        "    online_params = optax.apply_updates(state.online_params, updates)\n",
        "\n",
        "    # Update target network params as:\n",
        "    #   target_params <- ema * target_params + (1 - ema) * online_params\n",
        "    # You code here !\n",
        "    ema = self._target_ema\n",
        "    target_params = jax.tree_map(\n",
        "        lambda online, target: ema * target + (1 - ema) * online,\n",
        "        online_params,\n",
        "        state.target_params,\n",
        "    )\n",
        "\n",
        "    new_state = LearnerState(\n",
        "      online_params=online_params,\n",
        "      target_params=target_params,\n",
        "      opt_state=opt_state,\n",
        "    )\n",
        "\n",
        "    return new_state, loss\n",
        "\n",
        "  def observe(self, action: chex.Array, timestep: dm_env.TimeStep) -> None:\n",
        "    \"\"\"Updates the agent from the given observations.\n",
        "\n",
        "    Args:\n",
        "      action: action performed at time t.\n",
        "      timestep: timestep returned by the environment after \n",
        "    \"\"\"\n",
        "    # Create the transition.\n",
        "    transition = Transition(\n",
        "        # Current observation.\n",
        "        observation=self._observation,\n",
        "        # Action taken given that observation.\n",
        "        action=action,\n",
        "        # Result of taking the action.\n",
        "        reward=timestep.reward,\n",
        "        done=timestep.last(),\n",
        "        next_observation=timestep.observation,\n",
        "    )\n",
        "    # Add the transition to the replay buffer.\n",
        "    self._buffer.add(transition)\n",
        "    # Update the current observation.\n",
        "    self._observation = timestep.observation\n",
        "\n",
        "  def update(self) -> chex.Array | None:\n",
        "    \"\"\"Performs an update step if there is enough transitions in the buffer.\n",
        "    \n",
        "    Returns: DQN loss obtained when updating the online network or None if\n",
        "      there was not enough data.\n",
        "    \"\"\"\n",
        "    if self._buffer.size >= self._batch_size:\n",
        "      batch = self._buffer.sample_batch(self._batch_size)\n",
        "      self._learner_state, loss = self._update(self._learner_state, batch)\n",
        "      return loss\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Define the acting loop.\n",
        "\n",
        "def run_dqn_episode(\n",
        "    dqn_agent: DQNAgent,\n",
        "    env: dm_catch.Catch,\n",
        "    eval: bool,\n",
        ") -> float:\n",
        "  \"\"\"Runs a single episode of catch.\n",
        "\n",
        "  Args:\n",
        "    dqn_agent: agent to train or evaluate\n",
        "    env: the Catch environment the agent should interact with.\n",
        "    eval: evaluation mode.\n",
        "  Returns:\n",
        "    The total reward accumulated over the episode.\n",
        "  \"\"\"\n",
        "  # Reset any counts and start the environment.\n",
        "  timestep = env.reset()\n",
        "  dqn_agent.observe_first(timestep.observation)\n",
        "  total_reward = 0\n",
        "\n",
        "  # Run an episode.\n",
        "  while not timestep.last():\n",
        "\n",
        "    # Generate an action from the agent's policy and step the environment.\n",
        "    action = dqn_agent.select_action(timestep.observation[None, ...], eval)\n",
        "    timestep = env.step(action)\n",
        "    \n",
        "    # If the agent is training (not eval), add the transition to the replay\n",
        "    # buffer and do an update step.\n",
        "    if not eval:\n",
        "      dqn_agent.observe(action, timestep)\n",
        "\n",
        "    total_reward += timestep.reward\n",
        "\n",
        "  return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGnyMIUpFEeC"
      },
      "outputs": [],
      "source": [
        "#@title Train the DQN agent. { form-width: \"30%\" }\n",
        "\n",
        "num_episodes = 2_000\n",
        "batch_size = 32\n",
        "\n",
        "# Evaluation hyperparameters.\n",
        "num_eval_episodes = 10\n",
        "eval_every_period = 200\n",
        "\n",
        "# Bind a dummy observation to the init function so the agent doesn't have to.\n",
        "observation_spec = environment.observation_spec()\n",
        "dummy_observation = np.zeros(observation_spec.shape, observation_spec.dtype)\n",
        "init_params_fn = lambda rng: dqn_network.init(rng, dummy_observation[None, ...])\n",
        "\n",
        "# Create the agent.\n",
        "dqn_agent = DQNAgent(\n",
        "    init_params_fn=init_params_fn,\n",
        "    network_apply_fn=dqn_network.apply,\n",
        "    optimizer=optax.adam(learning_rate=3e-4),\n",
        "    gamma=0.9,\n",
        "    epsilon=0.3,\n",
        "    num_actions=environment.action_spec().num_values,\n",
        "    buffer_capacity=1_000,\n",
        "    batch_size=batch_size,\n",
        "    target_ema=0.99,\n",
        ")\n",
        "\n",
        "print(f\"Episode number:\\t| Average reward on {num_eval_episodes} eval episodes\")\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "# Initialize logged quantities.\n",
        "episodes = []\n",
        "all_rewards = []\n",
        "all_losses = []\n",
        "online_test_q_values = []\n",
        "target_test_q_values = []\n",
        "\n",
        "# Create a test batch of all possible initial observations.\n",
        "initial_glove_row = np.zeros((1, num_columns))\n",
        "initial_glove_row[:, num_columns // 2] = 1\n",
        "test_batch = np.concatenate(\n",
        "    [\n",
        "        np.zeros((num_columns, num_rows - 2, num_columns)),\n",
        "        np.eye(num_columns)[:, None, :],\n",
        "        num_columns * [initial_glove_row],\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  # Run a training episode and then a training step.\n",
        "  run_dqn_episode(\n",
        "      dqn_agent,\n",
        "      environment,\n",
        "      eval=False\n",
        "  )\n",
        "  loss = dqn_agent.update()\n",
        "\n",
        "  # Store some important diagnostic metrics to plot later.\n",
        "  all_losses.append(loss)\n",
        "  online_test_q_values.append(\n",
        "      dqn_network.apply(dqn_agent._learner_state.online_params, test_batch)\n",
        "  )\n",
        "  target_test_q_values.append(\n",
        "      dqn_network.apply(dqn_agent._learner_state.target_params, test_batch)\n",
        "  )\n",
        "\n",
        "  # Every once in a while, evaluate the greedy policy on a few episodes.\n",
        "  if episode % eval_every_period == 0:\n",
        "    reward = np.mean([\n",
        "        run_dqn_episode(dqn_agent, environment, eval=True)\n",
        "        for _ in range(num_eval_episodes)\n",
        "    ])\n",
        "    # Print how much reward the agent accumulated on average.\n",
        "    print(f\"\\t{episode}\\t|\\t{reward}\")\n",
        "    all_rewards.append(reward)\n",
        "    episodes.append(episode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Visualize the evaluation return and the training loss.\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axs[0].set_xlabel('Number of training episodes')\n",
        "axs[0].set_ylabel('Average return')\n",
        "axs[0].plot(episodes, all_rewards)\n",
        "axs[0].set_ylim([-1.1, 1.1])\n",
        "axs[1].set_xlabel('Number of updates')\n",
        "axs[1].set_ylabel('Average loss')\n",
        "axs[1].plot(np.asarray(all_losses).T)\n",
        "axs[1].set_ylim([0, 0.25]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Visualize the Q-values at all possible last steps.\n",
        "\n",
        "online_test_q_values = np.asarray(online_test_q_values)\n",
        "target_test_q_values = np.asarray(target_test_q_values)\n",
        "\n",
        "fig, axs = plt.subplots(2, num_columns, figsize=(num_columns * 5, 10), sharey=True, sharex=True)\n",
        "plt.ylim([-1.1, 1.1])\n",
        "axs[0, 0].set_ylabel('Online Q-values')\n",
        "axs[1, 0].set_ylabel('Target Q-values')\n",
        "axs[1, 2].set_xlabel('Number of online network updates')\n",
        "for i in range(num_columns):\n",
        "  if i == num_columns // 2 - 1:\n",
        "    should_play = 'left'\n",
        "  elif i == num_columns // 2:\n",
        "    should_play = 'no-op'\n",
        "  elif i == num_columns // 2 + 1:\n",
        "    should_play = 'right'\n",
        "  else:\n",
        "    should_play = None\n",
        "\n",
        "  axs[0, i].set_title(f'Should play {should_play}' if should_play else None)\n",
        "  axs[0, i].plot(online_test_q_values[:, i], label=['left', 'no-op', 'right'])\n",
        "  axs[1, i].plot(target_test_q_values[:, i])\n",
        "\n",
        "  if i == 0:\n",
        "    axs[0, i].legend()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
